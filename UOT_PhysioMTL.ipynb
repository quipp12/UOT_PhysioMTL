{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2kjw6wa1hx8"
      },
      "source": [
        "# PhysioMTL\n",
        "\n",
        "The code below attempts to reproduce work from the 2022 paper “PhysioMTL: Personalizing Physiological Patterns using Optimal Transport Multi-Task Regression” (Zhu et al. 2022) [1]. The paper outlines Physiological Multitask-Learning, or PhysioMTL, a proposed method for improving heart rate variability (HRV) interpretation developed by resources at Apple Machine Learning Resource, Carnegie Mellon University, University of Illinois Urbana-Champaign, and University of Michigan. HRV is used to measure cardiovascular health and detect acute illnesses, but is too sensitive to irrelevant factors such as hydration, sleep, stress, etc. (Acharya et al. 2006; European Society of Cardiology and Pacing Electrophysiology 1996) [2]. PhysioMTL attempts to denoise and normalize HRV data. Specifically, it learns to predict heart rate patterns at specific times of day based on demographics (age, gender, activity level, stress, etc).\n",
        "\n",
        "What makes PhysioMTL interesting is that it utilizes multi-task learning (MTL), treating patients as tasks. It then uses Optimal Transport (OT) methods to efficiently match data to unseen patients and adjust for data gaps. Overall, this is an early use of multi-task learning in healthcare.\n",
        "\n",
        "The specific claims of the original paper we are testing in this code are as follows:\n",
        "\n",
        "* PhysioMTL trained on a large sample set (80% of MMASH) outperforms (RMSE) the 5 baseline methods above\n",
        "* PhysioMTL trained on a very small data set (20% of MMASH) outperforms (RMSE) the 5 baseline methods above\n",
        "* PhysioMTL holds a more stable (lesser range) accuracy than the 5 baseline methods above across many data set training size (20%, 40%, 60%, and 80% of MMASH)\n",
        "\n",
        "Each claim will also be evaluated against the 5 baseline methods of Group Lasso, Multi-level Lasso, Dirty models, Multi-task Wasserstein, and Reweighted Multi-task Wasserstein.\n",
        "\n",
        "The following code will automatically download, process, train, and test the MMASH dataset and report results that support the claims presented in the paper. The notebook will generate an HRV prediction for a person who is 40 with a BMI of 25, sleeps 7 hours per day, has an activity level of 1.5 hours per day, and a stress level of 40. This prediction is also modeled in the original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj3umy8bSCl1"
      },
      "source": [
        "# Requirements & Imports\n",
        "\n",
        "This code is intended to be ran on `Python 3.9`.\n",
        "\n",
        "Multi-task regression library MuTaR is utilized to compare PhysioMTL evaluation with various other methods, including the following:\n",
        "\n",
        "* Group Lasso\n",
        "* Multi-Level Lasso\n",
        "* Dirty Model\n",
        "* MT Wasserstein\n",
        "* Reweighted MT Wasserstein\n",
        "* PhysioMTL\n",
        "\n",
        "Python Optimal Transport (POT) and HWCounter are used for reporting of computational requirements and comparison of efficiency among the compared methods. These external libraries, along with other standard Python libraries, are mentioned below with version requirements:\n",
        "\n",
        "* Numpy (1.22.4)\n",
        "* Pandas (1.5.3)\n",
        "* Matplotlib (3.7.1)\n",
        "* Pot (0.9.0)\n",
        "* Scipy (1.10.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FydUDOWvZ06d",
        "outputId": "313bcd8f-37c4-45f2-a131-5c1ae6341850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.26.4\n",
            "3.7.1\n",
            "1.11.4\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "print(pandas.__version__)\n",
        "import numpy\n",
        "print(numpy.__version__)\n",
        "import matplotlib\n",
        "print(matplotlib.__version__)\n",
        "import scipy\n",
        "print(scipy.__version__)\n",
        "# import pot\n",
        "# print(pot.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8guuC_e1SEdH",
        "outputId": "1d2b3361-b191-4a70-d6d9-b2aee6be357e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutar'...\n",
            "remote: Enumerating objects: 916, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 916 (delta 8), reused 16 (delta 7), pack-reused 899\u001b[K\n",
            "Receiving objects: 100% (916/916), 1.72 MiB | 3.40 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/mutar\n",
            "Obtaining file:///content/mutar\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mutar==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mutar==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.40.1 in /usr/local/lib/python3.10/dist-packages (from mutar==0.0.1) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.40.1->mutar==0.0.1) (0.41.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mutar==0.0.1) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mutar==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mutar==0.0.1) (3.5.0)\n",
            "Installing collected packages: mutar\n",
            "  Running setup.py develop for mutar\n",
            "Successfully installed mutar-0.0.1\n",
            "/content\n",
            "Collecting hwcounter\n",
            "  Downloading hwcounter-0.1.1.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: hwcounter\n",
            "  Building wheel for hwcounter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hwcounter: filename=hwcounter-0.1.1-cp310-cp310-linux_x86_64.whl size=11741 sha256=01dcf496834ba4e9e86e71e897dcc23a24dacb9a86e4172aa5e3a26d09be9556\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/2e/b6/a4a88b10b8eb8ce2c79dceefa7302437ebff769e08bb673a90\n",
            "Successfully built hwcounter\n",
            "Installing collected packages: hwcounter\n",
            "Successfully installed hwcounter-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hichamjanati/mutar\n",
        "%cd mutar\n",
        "!pip install -e .\n",
        "%cd /content\n",
        "\n",
        "# %pip install pot\n",
        "%pip install hwcounter\n",
        "\n",
        "import site\n",
        "site.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ol7BOso-ba48"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # !pip install --force-reinstall numpy==1.23.4\n",
        "# !pip install --force-reinstall pandas==1.5.3\n",
        "# # !pip install --force-reinstall matplotlib==3.7.1\n",
        "# # !pip install --force-reinstall scipy==1.10.1\n",
        "# !pip install pot==0.9.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PnavrIKiSHwm"
      },
      "outputs": [],
      "source": [
        "# Evaluation of computational requirements, hardware logging, etc.\n",
        "import platform\n",
        "import socket\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import psutil\n",
        "import logging\n",
        "\n",
        "# Regression methods\n",
        "import zipfile\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import ot\n",
        "import matplotlib.cm as cm\n",
        "import random\n",
        "import mutar\n",
        "from mutar import GroupLasso, DirtyModel, MTW, MultiLevelLasso, ReMTW\n",
        "from hwcounter import Timer, count, count_end\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lADdbxQldFh"
      },
      "source": [
        "## Identify System Resources\n",
        "\n",
        "These method definitions are used in various downstream classes to calculate precise computational requirements of both PhysioMTL and comparable methods. System information is printed and should be evaluated when comparing your results with results reported in our paper, as varation in CPU/memory and runtime can introduce considerable differences in CPU utilization. System requirements used in the creation of our paper are as follows:\n",
        "\n",
        "* Architecture: x86_64\n",
        "* RAM: 13GB\n",
        "* CPU Cores (Physical): 1\n",
        "* CPU Cores (Logical): 2\n",
        "* Platform: Linux\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ebo7-1rGlfdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa42e5a5-3bae-4fce-d98f-40f63ba9a082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Platform': 'Linux', 'Architecture': 'x86_64', 'Processor': 'x86_64', 'RAM': '12.675 GB', 'CPU Cores (Physical)': 1, 'CPU Cores (Total)': 2, 'Memory (Total)': '12.67GB', 'Memory (Available)': '10.96GB', 'Memory (Used)': '1.40GB', 'Memory Utilization': '13.5%'}\n"
          ]
        }
      ],
      "source": [
        "def get_size(bytes):\n",
        "    bucket = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < bucket:\n",
        "            return f\"{bytes:.2f}{unit}B\"\n",
        "        bytes /= bucket\n",
        "\n",
        "def get_memory_utilization():\n",
        "    svmem = psutil.virtual_memory()\n",
        "    return {\n",
        "        'Memory (Total)': f'{get_size(svmem.total)}',\n",
        "        'Memory (Available)': f'{get_size(svmem.available)}',\n",
        "        'Memory (Used)': f'{get_size(svmem.used)}',\n",
        "        'Memory Utilization': f'{svmem.percent}%'\n",
        "    }\n",
        "\n",
        "def get_system_info():\n",
        "    svmem = psutil.virtual_memory()\n",
        "    return {\n",
        "        'Platform': platform.system(),\n",
        "        'Architecture': platform.machine(),\n",
        "        'Processor': platform.processor(),\n",
        "        'RAM': f'{round(psutil.virtual_memory().total / (1024 ** 3), 3)} GB',\n",
        "        'CPU Cores (Physical)': psutil.cpu_count(logical=False),\n",
        "        'CPU Cores (Total)': psutil.cpu_count(logical=True)\n",
        "    } | get_memory_utilization()\n",
        "\n",
        "def get_cpu_seconds(cycles):\n",
        "    return cycles / psutil.cpu_count(logical=True) / 1000000000\n",
        "\n",
        "print(get_system_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRfcLnh9dT3M"
      },
      "source": [
        "# MMASH Data Set\n",
        "\n",
        "The MMASH dataset contains 24-hour psycho-physiological data from 22 healthy adults. It includes data on respiratory rate, sleep quality, physical activity, anxiety, stress, and emotions Multilevel Monitoring of Activity and Sleep in Healthy People (MMASH) [3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH_FJcJPaWAA"
      },
      "source": [
        "##Download MMASH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ObvkW1nbdWX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4b00d3-1fdc-4c1c-8020-0006a1bcb42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2024-06-26 13:06:15--  https://physionet.org/files/mmash/1.0.0/MMASH.zip\n",
            "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
            "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24649620 (24M) [application/zip]\n",
            "Saving to: ‘MMASH/mmash.zip’\n",
            "\n",
            "MMASH/mmash.zip     100%[===================>]  23.51M   547KB/s    in 44s     \n",
            "\n",
            "2024-06-26 13:07:00 (546 KB/s) - ‘MMASH/mmash.zip’ saved [24649620/24649620]\n",
            "\n",
            "FINISHED --2024-06-26 13:07:00--\n",
            "Total wall clock time: 45s\n",
            "Downloaded: 1 files, 24M in 44s (546 KB/s)\n"
          ]
        }
      ],
      "source": [
        "!mkdir MMASH\n",
        "!mkdir Data\n",
        "!mkdir Data/figures\n",
        "!wget -O MMASH/mmash.zip -r -N -c -np https://physionet.org/files/mmash/1.0.0/MMASH.zip\n",
        "\n",
        "with zipfile.ZipFile('MMASH/mmash.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('MMASH')\n",
        "\n",
        "!rm MMASH/mmash.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7q-QrhEcrnQ"
      },
      "source": [
        "##Preprocess MMASH\n",
        "\n",
        "Preprocessing of the MMASH dataset is by converting respiratory rate to HRV in 5 minute intervals, omission of instances with a z-score over a threshold (2.5 by default). We also perform additional data cleansing to match behavior followed by the authors of the original paper, which included ignoring patient 4 (due to extreme HRV measurements) and populating missing sleep and age data on two other patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "co0zTsuNpSbA"
      },
      "outputs": [],
      "source": [
        "# Scripting constants\n",
        "BASELINE_DATES = {1: \"2020-08-01\", 2: \"2020-08-02\"}\n",
        "RANDOM_STATE = 12345\n",
        "USER_FILE_PREFIX = \"/content/MMASH/DataPaper/user_\"\n",
        "TASKS = range(1, 23)\n",
        "\n",
        "# Data set variables\n",
        "RELEVANT_ACTIVITY = [4, 5]\n",
        "USER_INFO_FIELDS = [\"Age\", \"Height\", \"Weight\"]\n",
        "OMIT_TASKS = [4,]\n",
        "OVERWRITES = {3: [5,60], 11: [4,6.5], 18: [0,22]}\n",
        "RAD_HOUR = 2.0 * np.pi / 24\n",
        "\n",
        "# Pre-processing hyperparameters\n",
        "IBIS_UPPER = 2.1\n",
        "IBIS_LOWER = 0.3\n",
        "FREQUENCY = \"5min\"\n",
        "NOISE_UPPER = 55.0\n",
        "NOISE_SAMPLE_PERCENT = 0.85"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-e1PcHsJcueP"
      },
      "outputs": [],
      "source": [
        "class MMASH:\n",
        "    def __init__(self, tasks, max_zscore, max_noise=NOISE_UPPER):\n",
        "        self.max_zscore = max_zscore\n",
        "        if self.max_zscore is None:\n",
        "            self.max_zscore = 999999\n",
        "        self.max_noise = max_noise\n",
        "        self.X, self.S, self.Y = [], [], []\n",
        "        self.start_processing_cycles = count()\n",
        "\n",
        "        for i in tasks:\n",
        "            i_start = count()\n",
        "            user_folder = USER_FILE_PREFIX + str(i)\n",
        "\n",
        "            sleep_value = self.get_sleep_data(pd.read_csv(user_folder + \"/sleep.csv\"))\n",
        "            activity_value = self.get_activity_data(pd.read_csv(user_folder + \"/Activity.csv\", header=0))\n",
        "            user_info_value = self.get_user_info_data(pd.read_csv(user_folder + \"/user_info.csv\", header=0))\n",
        "            questionnaire_value = self.get_questionnaire_data(pd.read_csv(user_folder + \"/questionnaire.csv\", header=0))\n",
        "            rr_value = self.get_rr_data(pd.read_csv(user_folder + \"/RR.csv\", header=0))\n",
        "\n",
        "            t_hour = rr_value[\"t_hour\"].values\n",
        "            s_vector = np.hstack([user_info_value, activity_value, sleep_value, questionnaire_value])\n",
        "            if i in OVERWRITES:\n",
        "                s_vector[OVERWRITES[i][0]] = OVERWRITES[i][1]\n",
        "\n",
        "            self.X.append(np.asarray([np.sin(RAD_HOUR * t_hour), np.cos(RAD_HOUR * t_hour), np.ones(t_hour.shape[0], )]).T)\n",
        "            self.S.append(s_vector.reshape(-1, 1))\n",
        "            self.Y.append(rr_value[\"value\"].values.reshape((-1, 1)))\n",
        "\n",
        "            i_cycles = count_end() - i_start\n",
        "            print(f'{i}: preprocessed task, y-mean = {rr_value[\"value\"].mean()}, elapsed time: {i_cycles} cycles ({get_cpu_seconds(i_cycles)} seconds)')\n",
        "\n",
        "\n",
        "\n",
        "        # reshaped_X = []\n",
        "\n",
        "        # # Loop through each array in self.X\n",
        "        # for x in self.X:\n",
        "        #     # If the number of rows is greater than 176, truncate it\n",
        "        #     if x.shape[0] > 176:\n",
        "        #         reshaped_X.append(x[:176, :])\n",
        "        #     # If the number of rows is less than 176, pad it\n",
        "        #     else:\n",
        "        #         # Calculate how many rows are needed to reach 176\n",
        "        #         pad_size = 176 - x.shape[0]\n",
        "        #         # Create a padding array of zeros with the appropriate shape\n",
        "        #         pad_array = np.zeros((pad_size, x.shape[1]))\n",
        "        #         # Concatenate the original array with the padding array\n",
        "        #         reshaped_X.append(np.vstack((x, pad_array)))\n",
        "\n",
        "        # # Optionally, assign back to self.X if you want to replace the original list\n",
        "        # self.X = reshaped_X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # reshaped_Y = []\n",
        "\n",
        "        # # Loop through each array in self.Y\n",
        "        # for y in self.Y:\n",
        "        #     # If the number of rows is greater than 176, truncate it\n",
        "        #     if y.shape[0] > 176:\n",
        "        #         reshaped_Y.append(y[:176, :])\n",
        "        #     # If the number of rows is less than 176, pad it\n",
        "        #     else:\n",
        "        #         # Calculate how many rows are needed to reach 176\n",
        "        #         pad_size = 176 - y.shape[0]\n",
        "        #         # Create a padding array of zeros with the appropriate shape\n",
        "        #         pad_array = np.zeros((pad_size, y.shape[1]))\n",
        "        #         # Concatenate the original array with the padding array\n",
        "        #         reshaped_Y.append(np.vstack((y, pad_array)))\n",
        "\n",
        "        # # Optionally, assign back to self.Y if you want to replace the original list\n",
        "        # self.Y = reshaped_Y\n",
        "\n",
        "        print([x.shape for x in self.X])\n",
        "        print([x.shape for x in self.S])\n",
        "        print([x.shape for x in self.Y])\n",
        "        for i in range(len(self.X)):\n",
        "            self.X[i]=np.array(self.X[i])\n",
        "        for i in range(len(self.Y)):\n",
        "            self.Y[i]=np.array(self.Y[i])\n",
        "        for i in range(len(self.S)):\n",
        "            self.S[i]=np.array(self.S[i])\n",
        "        print(type(self.X),type(self.X[0]))\n",
        "\n",
        "\n",
        "        # self.X = np.asarray(self.X)\n",
        "        # self.S = np.asarray(self.S)\n",
        "        # self.Y = np.asarray(self.Y)\n",
        "        self.elapsed_processing_cycles = count_end() - self.start_processing_cycles\n",
        "        print(f'finished processing mmash data set, elapsed cycles: {self.elapsed_processing_cycles} cycles ({get_cpu_seconds(self.elapsed_processing_cycles)} seconds)')\n",
        "\n",
        "    def get_sleep_data(self, data):\n",
        "        return data[\"Total Minutes in Bed\"].sum() / 60\n",
        "\n",
        "    def get_activity_data(self, data):\n",
        "        data = data.copy().dropna()\n",
        "        activity = lambda x: pd.to_timedelta(x[\"End\"] + \":00\") - pd.to_timedelta(x[\"Start\"] + \":00\")\n",
        "        data[\"time_start_pd\"] = data.apply(activity, axis=1)\n",
        "        data[\"time_end_pd\"] = data.apply(activity, axis=1)\n",
        "        data[\"time_last_hour\"] = data[\"time_end_pd\"].apply(lambda x: x.seconds / 3600)\n",
        "        data = data.loc[data[\"Activity\"].isin(RELEVANT_ACTIVITY)]\n",
        "        return data[\"time_last_hour\"].sum() if not data.empty else 0\n",
        "\n",
        "    def get_user_info_data(self, data):\n",
        "        data = data[USER_INFO_FIELDS].loc[0].values\n",
        "        return [data[0], data[1] / 100.0, data[2]]\n",
        "\n",
        "    def get_questionnaire_data(self, data):\n",
        "        return data[\"Daily_stress\"].loc[0]\n",
        "\n",
        "    def get_rr_data(self, data):\n",
        "        data[\"new_time\"] = data.apply(lambda ds: pd.to_datetime(BASELINE_DATES.get(ds[\"day\"], BASELINE_DATES[2]) + \" \" + str(ds[\"time\"])), axis=1)\n",
        "        data['ibi_s'] = [x if x < IBIS_UPPER and x > IBIS_LOWER else np.nan for x in data['ibi_s']]\n",
        "        data = [w for _, w in data.dropna().groupby(pd.Grouper(key='new_time', freq=FREQUENCY)) if not w.empty]\n",
        "        rr_value = pd.DataFrame({\n",
        "            \"new_time\": [x[\"new_time\"].mean() for x in data],\n",
        "            \"value\": [np.std(1000 * x[\"ibi_s\"], ddof=1) for x in data]\n",
        "        })\n",
        "        no_noise = rr_value[\"value\"] < self.max_noise\n",
        "        no_noise = rr_value[no_noise].sample(int(NOISE_SAMPLE_PERCENT * no_noise.sum()), random_state=RANDOM_STATE)\n",
        "        rr_value = rr_value.drop(no_noise.index)\n",
        "        rr_value = rr_value.loc[np.abs(stats.zscore(rr_value[\"value\"])) < self.max_zscore]\n",
        "        rr_value = rr_value.loc[rr_value[\"value\"] > 12]\n",
        "        rr_value[\"t_hour\"] = rr_value[\"new_time\"].apply(lambda x: (x - pd.Timestamp(BASELINE_DATES[1] + \" 00:00:00\")).total_seconds() / 3600)\n",
        "        return rr_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TlBk8vCG1PeG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2bw0kzOuunVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26171340-7385-493e-b90c-f78d865bfe3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: preprocessed task, y-mean = 85.56476822678721, elapsed time: 24020697474 cycles (12.010348737 seconds)\n",
            "2: preprocessed task, y-mean = 100.33561841324313, elapsed time: 16547829240 cycles (8.27391462 seconds)\n",
            "3: preprocessed task, y-mean = 76.47079460368347, elapsed time: 22157179106 cycles (11.078589553 seconds)\n",
            "5: preprocessed task, y-mean = 90.258288824083, elapsed time: 18260483648 cycles (9.130241824 seconds)\n",
            "6: preprocessed task, y-mean = 88.1846116338921, elapsed time: 22268711848 cycles (11.134355924 seconds)\n",
            "7: preprocessed task, y-mean = 85.14250546385614, elapsed time: 23031030042 cycles (11.515515021 seconds)\n",
            "8: preprocessed task, y-mean = 96.14583988348869, elapsed time: 19757040754 cycles (9.878520377 seconds)\n",
            "9: preprocessed task, y-mean = 88.87771062928078, elapsed time: 20004884304 cycles (10.002442152 seconds)\n",
            "10: preprocessed task, y-mean = 96.38566520289122, elapsed time: 17597487180 cycles (8.79874359 seconds)\n",
            "11: preprocessed task, y-mean = 87.05496543244831, elapsed time: 19767368318 cycles (9.883684159 seconds)\n",
            "12: preprocessed task, y-mean = 71.64783622577323, elapsed time: 19136344142 cycles (9.568172071 seconds)\n",
            "13: preprocessed task, y-mean = 75.48997318086856, elapsed time: 20667859500 cycles (10.33392975 seconds)\n",
            "14: preprocessed task, y-mean = 72.4216758657783, elapsed time: 20862447304 cycles (10.431223652 seconds)\n",
            "15: preprocessed task, y-mean = 89.94723825334468, elapsed time: 25099198190 cycles (12.549599095 seconds)\n",
            "16: preprocessed task, y-mean = 90.6241899400592, elapsed time: 17921752082 cycles (8.960876041 seconds)\n",
            "17: preprocessed task, y-mean = 84.65279849400704, elapsed time: 23061317542 cycles (11.530658771 seconds)\n",
            "18: preprocessed task, y-mean = 97.55197871468567, elapsed time: 20366103938 cycles (10.183051969 seconds)\n",
            "19: preprocessed task, y-mean = 93.08070948073929, elapsed time: 18084536428 cycles (9.042268214 seconds)\n",
            "20: preprocessed task, y-mean = 81.19209160183755, elapsed time: 19035073014 cycles (9.517536507 seconds)\n",
            "21: preprocessed task, y-mean = 77.00527424006555, elapsed time: 20835273718 cycles (10.417636859 seconds)\n",
            "22: preprocessed task, y-mean = 83.18474159704846, elapsed time: 15746063892 cycles (7.873031946 seconds)\n",
            "[(226, 3), (204, 3), (204, 3), (239, 3), (213, 3), (194, 3), (191, 3), (192, 3), (218, 3), (253, 3), (183, 3), (184, 3), (179, 3), (234, 3), (206, 3), (254, 3), (251, 3), (190, 3), (176, 3), (202, 3), (229, 3)]\n",
            "[(6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1)]\n",
            "[(226, 1), (204, 1), (204, 1), (239, 1), (213, 1), (194, 1), (191, 1), (192, 1), (218, 1), (253, 1), (183, 1), (184, 1), (179, 1), (234, 1), (206, 1), (254, 1), (251, 1), (190, 1), (176, 1), (202, 1), (229, 1)]\n",
            "<class 'list'> <class 'numpy.ndarray'>\n",
            "finished processing mmash data set, elapsed cycles: 424282859210 cycles (212.141429605 seconds)\n"
          ]
        }
      ],
      "source": [
        "tasks = [i for i in TASKS if i not in OMIT_TASKS]\n",
        "mmash = MMASH(tasks, max_zscore=2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5CHo7_kaZ-D"
      },
      "source": [
        "# PhysioMTL Model Definition\n",
        "\n",
        "The below PhysioMTL model is a multi-task learning model that treats every known patient as a task but does not follow a standard neural network pattern such as GNN or RNN. The algorithm learns a transformation matrix that maps the input features of each subject to a common feature space, and then learns a weight matrix that maps the common feature space to the output space of each subject.\n",
        "\n",
        "The goal is to minimize the distance between predictions and true outputs across all subjects. The parameters are as follows:\n",
        "* Input data (X) has 1 parameter per task for time.\n",
        "* Source tasks (S) have 6 parameters for sleep, activity, stress, age, height, and weight.\n",
        "* And target tasks (Y) have one parameter of HRV.\n",
        "\n",
        "During the learning process for optimization/loss, the model first uses input data and target tasks to solve a weight matrix using linear regression. When it constructs cost and mapping matrices from source tasks, it uses weighted root mean squared error as a cost or objective function. Then it uses optimal transport (OT) to build a plan using the Sinkhorn algorithm, and a transformation matrix that maps demographic features to HRV. Using gradient descent, it alternates between optimizing the transformation matrix and OT plan until convergence. Once the transformation matrix is learned, the model can predict HRV data for new sets of demographic and time-of-day features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bvb55FWlBeMD"
      },
      "outputs": [],
      "source": [
        "T_GRADIENT_THRESHOLD = 1e-6\n",
        "W_GRADIENT_THRESHOLD = 1e-6\n",
        "COST_FUNC = lambda x, y: np.sqrt(np.mean(np.square(np.dot([1, 10., 1., 10, 1., 1.], x - y))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QWdABkiar25i"
      },
      "outputs": [],
      "source": [
        "import cvxpy as cp\n",
        "\n",
        "from copy import copy\n",
        "\n",
        "\n",
        "smallest_float = np.nextafter(np.float16(0), np.float16(1))\n",
        "float_type = np.double\n",
        "\n",
        "\n",
        "def compute_B(C, u, v, eta):\n",
        "\n",
        "    return np.exp((u + v.T - C) / eta)\n",
        "\n",
        "def sinkhorn_uot(C, a, b, eta=1.0, tau1=1.0, tau2=1.0, k=100, compute_optimal=False):\n",
        "    \"\"\"\n",
        "    Sinkhorn algorithm for entropic-regularized Unbalanced Optimal Transport.\n",
        "\n",
        "    :param C:\n",
        "    :param a:\n",
        "    :param b:\n",
        "    :param eta:\n",
        "    :param tau1:\n",
        "    :param tau2:\n",
        "    :param k:\n",
        "    :param epsilon:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    output = {\n",
        "        \"u\": list(),\n",
        "        \"v\": list(),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    # Initialization\n",
        "    u = np.zeros_like(a).astype(float_type)\n",
        "    v = np.zeros_like(b).astype(float_type)\n",
        "\n",
        "    output[\"u\"].append(copy(u))\n",
        "    output[\"v\"].append(copy(v))\n",
        "\n",
        "    for i in range(int(k)):\n",
        "        u_old = copy(u)\n",
        "        v_old = copy(v)\n",
        "        B = compute_B(C, u, v, eta)\n",
        "\n",
        "        if i % 2 == 0:\n",
        "            Ba = B.sum(axis=1).reshape(-1, 1)\n",
        "            u = (u / eta + np.log(a) - np.log(Ba)) * (tau1 * eta / (eta + tau1))\n",
        "        else:\n",
        "            Bb = B.sum(axis=0).reshape(-1, 1)\n",
        "            v = (v / eta + np.log(b) - np.log(Bb)) * (tau2 * eta / (eta + tau2))\n",
        "\n",
        "        output[\"u\"].append(copy(u))\n",
        "        output[\"v\"].append(copy(v))\n",
        "\n",
        "\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dict_OT={\n",
        "    \"OT time\":list(),\n",
        "    \"OT converge tim\":list(),\n",
        "    \"PhysioMTL Mean\":list(),\n",
        "    \"PhysioMTL OTS td\":list(),\n",
        "       }\n",
        "data_dict_UOT={\n",
        "    \"UOT time\":list(),\n",
        "    \"UOT converge tim\":list(),\n",
        "    \"PhysioMTL UOT Mean\":list(),\n",
        "    \"PhysioMTL UOT Std\":list(),\n",
        "       }\n",
        "data_dict_other={\n",
        "       \"Percentage\": list(),\n",
        "       \"Group Lasso Mean\": list(), \"Group Lasso Std\": list(),\n",
        "       \"MLevel Lasso Mean\": list(),\"MLevel Lasso Std\": list(),\n",
        "       \"Dirty Model Mean\": list(), \"Dirty Model Std\": list(),\n",
        "       \"MT Was Mean\": list(), \"MT Was Std\": list(),\n",
        "       \"MT Was Rewgt Mean\": list(), \"MT Was Rewgt Std\":list()\n",
        "\n",
        "       }\n",
        "output={\n",
        "        'parameters_OT':{ \"converge_iterations_OT\":list(),\n",
        "                          \"momentum\":list()\n",
        "                        },\n",
        "        'parameters_UOT':{ \"converge_iterations_UOT\":list(),\n",
        "                           \"momentum\":list(),\n",
        "                           \"eta\":list(),\n",
        "                           \"tau\":list(),\n",
        "                           \"k\":list()\n",
        "                        },\n",
        "        'OT_result':data_dict_OT,\n",
        "        'UOT_result':data_dict_UOT,\n",
        "        }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ILCB21M7MQvn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4N8BVtViacpx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "class PhysioMTL:\n",
        "    def __init__(self, cost_func, iterations=50, t_learn_rate=0.055, t_iterations=200, w_learn_rate=5*1e-9, w_iterations=50, alpha=0.1, epsilon=1e-4, lambda_=5, sigma=10, verbose=False):\n",
        "        self.cost_func = cost_func\n",
        "        self.iterations = iterations\n",
        "        self.t_learn_rate = t_learn_rate\n",
        "        self.t_iterations = t_iterations\n",
        "        self.w_learn_rate = w_learn_rate\n",
        "        self.w_iterations = w_iterations\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.lambda_ = lambda_   #tunable\n",
        "        self.sigma = sigma\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, X, S, Y,Momentum=True):\n",
        "        self.start_fit_cycles = count()\n",
        "        self.S = S\n",
        "        W = self.solve_w_by_linear_regression(X, Y)\n",
        "        C, K = self.get_cost_matrix(S)\n",
        "        Pi, count1 = self.solve_ot_plan(C)\n",
        "        T=len(C)\n",
        "        fracs = np.ones(T) / T\n",
        "        self.coef_ = self.W, self.T = self.converge_fw_gradient_descent(X, Y, W, K, Pi,Momentum,UOT=False)\n",
        "        self.end_fit_cycles = count_end() - self.start_fit_cycles\n",
        "        if self.verbose:\n",
        "          print(f'PhysioMTL fit completed in {self.end_fit_cycles} cycles ({get_cpu_seconds(self.end_fit_cycles)} seconds)')\n",
        "\n",
        "\n",
        "    def fit_uot(self, X, S, Y,Momentum=True):\n",
        "        self.start_fit_cycles = count()\n",
        "        self.S = S\n",
        "        W = self.solve_w_by_linear_regression(X, Y)\n",
        "        C, K = self.get_cost_matrix(S)\n",
        "        Pi = self.solve_uot_plan(C, eps=0.1, tau=3,k=30) #tune tau\n",
        "        self.coef_ = self.W, self.T = self.converge_fw_gradient_descent(X, Y, W, K, Pi,Momentum,UOT=True )\n",
        "        self.end_fit_cycles = count_end() - self.start_fit_cycles\n",
        "        if self.verbose:\n",
        "          print(f'PhysioMTL fit completed in {self.end_fit_cycles} cycles ({get_cpu_seconds(self.end_fit_cycles)} seconds)')\n",
        "\n",
        "\n",
        "    def solve_w_by_linear_regression(self, X, Y):\n",
        "        return np.concatenate([Y[i].T @ x @ np.linalg.inv(x.T @ x) for i, x in enumerate(X)]).T\n",
        "\n",
        "    def get_cost_matrix(self, S):\n",
        "        C = np.asarray([[self.cost_func(i, j) for j in S] for i in self.S])\n",
        "        K = np.exp(-C / (2 * self.sigma**2))\n",
        "        return C, K\n",
        "\n",
        "    def solve_ot_plan(self, C):\n",
        "        T = len(C)\n",
        "        fracs = np.ones(T) / T\n",
        "        transport_matrix = np.exp(-self.lambda_ * C)\n",
        "        transport_matrix /= transport_matrix.sum()\n",
        "        u = np.zeros(T)\n",
        "\n",
        "        count=0\n",
        "        while np.max(np.abs(u - transport_matrix.sum(1))) > self.epsilon:\n",
        "            u = transport_matrix.sum(1)\n",
        "            transport_matrix *= fracs / u[:, None]\n",
        "            transport_matrix *= fracs / transport_matrix.sum(0)[None, :]\n",
        "            count+=1\n",
        "        return transport_matrix, count\n",
        "\n",
        "\n",
        "    def solve_uot_plan(self, C, eps=0.1, tau=1, k=200):\n",
        "\n",
        "      T = len(C)\n",
        "      fracs = np.ones(T) / T\n",
        "      eta=0.7\n",
        "      uot_output = sinkhorn_uot(C, fracs , fracs, eta= eta, tau1=tau, tau2=tau, k= k, compute_optimal=True)\n",
        "      u = uot_output[\"u\"][-1]\n",
        "      v = uot_output[\"v\"][-1]\n",
        "      transport_matrix = compute_B(C, u, v, eta)\n",
        "      return transport_matrix\n",
        "\n",
        "\n",
        "    def converge_fw_gradient_descent(self, X, Y, W, K, Pi,Momentum,UOT):\n",
        "        N = len(X)\n",
        "        Z = np.zeros((W.shape[0], K.shape[0]))\n",
        "        T = np.zeros((W.shape[0], K.shape[0]))\n",
        "        norm_last = 1e6\n",
        "        if UOT==True:\n",
        "           count=450\n",
        "        else:\n",
        "           count=1500\n",
        "        if Momentum ==True:\n",
        "\n",
        "              norm_last = 1e6\n",
        "              for t in range(count):\n",
        "                grad = [Pi[i, j] * (W[:, i:i+1] - T @ K[:, j:j+1]) @ K[:, j:j+1].T for i in range(N) for j in range(N)]\n",
        "                grad = Z - self.alpha * 2 * sum(grad)\n",
        "                norm = np.sum(np.square(grad))\n",
        "                if np.abs(norm_last - norm) < T_GRADIENT_THRESHOLD:\n",
        "                    break\n",
        "                norm_last = norm\n",
        "                if t==0:\n",
        "                    v_t=0.011*grad\n",
        "                else:\n",
        "                    v_t=0.95*v_t+0.011* grad\n",
        "\n",
        "                T = T - v_t\n",
        "\n",
        "\n",
        "            #print(iteration, \": T gradient F-norm:\", norm)\n",
        "\n",
        "              norm_last = 1e6\n",
        "              for w in range(count):\n",
        "                grad = np.zeros_like(W)\n",
        "                for t in range(N):\n",
        "                    grad[:, t:t+1] = ((W[:, t:t+1].T @ X[t].T - Y[t].T) @ (X[t]) - self.alpha * sum([(T @ K[:, tt:tt+1] - W[:, tt:tt + 1]).T for tt in range(N)])).T\n",
        "                norm = np.sum(np.square(grad))\n",
        "                if np.abs(norm_last - norm) < W_GRADIENT_THRESHOLD:\n",
        "                    break\n",
        "                norm_last = norm\n",
        "                if w==0:\n",
        "                    v_t=1e-3*5* grad\n",
        "                else:\n",
        "                    v_t=0.9*v_t+1e-3*5* grad\n",
        "\n",
        "                W = W - v_t\n",
        "\n",
        "\n",
        "            #print(iteration, \": W gradient F-norm:\", norm)\n",
        "        if Momentum==False:\n",
        "              norm_last = 1e6\n",
        "              for t in range(count):\n",
        "                grad = [Pi[i, j] * (W[:, i:i+1] - T @ K[:, j:j+1]) @ K[:, j:j+1].T for i in range(N) for j in range(N)]\n",
        "                grad = Z - self.alpha * 2 * sum(grad)\n",
        "                norm = np.sum(np.square(grad))\n",
        "                if np.abs(norm_last - norm) < T_GRADIENT_THRESHOLD:\n",
        "                    break\n",
        "                norm_last = norm\n",
        "\n",
        "\n",
        "                T=T-self.t_learn_rate*grad\n",
        "            #print(iteration, \": T gradient F-norm:\", norm)\n",
        "\n",
        "              norm_last = 1e6\n",
        "              for w in range(count):\n",
        "                grad = np.zeros_like(W)\n",
        "                for t in range(N):\n",
        "                    grad[:, t:t+1] = ((W[:, t:t+1].T @ X[t].T - Y[t].T) @ (X[t]) - self.alpha * sum([(T @ K[:, tt:tt+1] - W[:, tt:tt + 1]).T for tt in range(N)])).T\n",
        "                norm = np.sum(np.square(grad))\n",
        "                if np.abs(norm_last - norm) < W_GRADIENT_THRESHOLD:\n",
        "                    break\n",
        "                norm_last = norm\n",
        "\n",
        "\n",
        "\n",
        "                W=W-self.w_learn_rate*grad\n",
        "            #print(iteration, \": W gradient F-norm:\", norm)\n",
        "\n",
        "\n",
        "        return W, T\n",
        "\n",
        "    def predict(self, X, S):\n",
        "        self.start_predict_cycles = count()\n",
        "        if X is None and S is None:\n",
        "            return [x @ self.W[:, i:i+1] for i, x in enumerate(self.X)]\n",
        "\n",
        "        _, K = self.get_cost_matrix(S)\n",
        "        K = [K[:, i:i+1] for i in range(len(S))]\n",
        "        self.end_predict_cycles = count_end() - self.start_predict_cycles\n",
        "        if self.verbose:\n",
        "            print(f'PhysioMTL predict completed in {self.end_predict_cycles} cycles ({get_cpu_seconds(self.end_predict_cycles)} seconds)')\n",
        "\n",
        "        return [x @ self.T @ K[i] for i, x in enumerate(X)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyc3QTY9a6p8"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "S5aoF39bC4A2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1bRkNOrva9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc49ecb-5deb-4d49-dc29-015c2ed73928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhysioMTL fit completed in 13894605620 cycles (6.94730281 seconds)\n",
            "PhysioMTL fit completed in 5797469508 cycles (2.898734754 seconds)\n"
          ]
        }
      ],
      "source": [
        "# model = PhysioMTL(COST_FUNC, verbose=True)\n",
        "# model.fit(mmash.X, mmash.S, mmash.Y)\n",
        "# import time\n",
        "\n",
        "model_ot = PhysioMTL(COST_FUNC, verbose=True)\n",
        "model_ot.fit(mmash.X, mmash.S, mmash.Y)\n",
        "\n",
        "model_uot = PhysioMTL(COST_FUNC, verbose=True)\n",
        "model_uot.fit_uot(mmash.X, mmash.S, mmash.Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h_8EzJDaw9O"
      },
      "source": [
        "# Sample PhysioMTL Prediction\n",
        "\n",
        "Perform a sample prediction against the trained model, and plot the predicted HRV values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMzrykuPYt3P"
      },
      "outputs": [],
      "source": [
        "S_ = np.array([23., 1.80, 85., 1., 7., 20.]).reshape(-1, 1)\n",
        "\n",
        "T_ = np.linspace(8, 36, 30)\n",
        "X_ = np.asarray([np.sin(RAD_HOUR * T_), np.cos(RAD_HOUR * T_), np.ones(30, )]).T\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ticks = [[9, '9:00'], [12, '12:00'], [15, '15:00'], [18, '18:00'], [21, '21:00'], [24, '24:00'], [27, '3:00'], [30, '6:00'], [33, '9:00'], [36, '12:00']]\n",
        "ax.set_xticks([x[0] for x in ticks])\n",
        "ax.set_xticklabels([x[1] for x in ticks], rotation=30)\n",
        "ax.set_ylabel(\"HRV\")\n",
        "ax.set_xlabel(\"Time\")\n",
        "\n",
        "ax.plot(T_, model_uot.predict([X_], [S_])[0])\n",
        "ax.plot(T_, model_ot.predict([X_], [S_])[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSnJSI9ddzRT"
      },
      "source": [
        "# Model Performance\n",
        "\n",
        "The following evaluation methods are used to both assess performance of our model and address various claims made by the authors of the original PhysioMTL paper.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79HKWnlDE607"
      },
      "source": [
        "## Evaluation Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FifZ5tEJeFZg"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_percentages(X, S, Y, percentages, compare_alpha=0.9, physio_alpha=0.1, ot_gd=True, ot_gdm=True, uot_gd=True, uot_gdm=True , iterations=20, cost_func=COST_FUNC):\n",
        "    for p in percentages:\n",
        "        start_evaluation_time = count()\n",
        "        evaluate_all(X, S, Y, p, compare_alpha, physio_alpha, iterations, cost_func,ot_gd=True, ot_gdm=True, uot_gd=True, uot_gdm=True,)\n",
        "        end_evaluation_time = count_end() - start_evaluation_time\n",
        "        print('    Evaluation:')\n",
        "        print(f'         iterations: {iterations}')\n",
        "        print(f'         elapsed time: {end_evaluation_time} cycles ({get_cpu_seconds(end_evaluation_time)} seconds)')\n",
        "        print('')\n",
        "\n",
        "def evaluate_all(X, S, Y, percentage, compare_alpha, physio_alpha, iterations, cost_func,ot_gd=True, ot_gdm=True, uot_gd=True, uot_gdm=True,):\n",
        "    mse_ot_gd = []\n",
        "    mse_glasso = []\n",
        "    mse_mlasso = []\n",
        "    mse_dirty = []\n",
        "    mse_mtw = []\n",
        "    mse_remtw = []\n",
        "    mse_uot_gd= []\n",
        "    mse_uot_gdm= []\n",
        "    mse_ot_gdm = []\n",
        "    time_ot_gd=[]\n",
        "    time_ot_gdm=[]\n",
        "    time_uot_gd=[]\n",
        "    time_uot_gdm=[]\n",
        "\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        cutoff = int(percentage * len(S))\n",
        "        idx = list(range(0, len(S)))\n",
        "        random.shuffle(idx)\n",
        "        train_index, test_index = idx[:cutoff], idx[cutoff:]\n",
        "        X_train = [X[i] for i in train_index]\n",
        "        S_train = [S[i] for i in train_index]\n",
        "        Y_train = [Y[i] for i in train_index]\n",
        "        X_test = [X[i] for i in test_index]\n",
        "        S_test = [S[i] for i in test_index]\n",
        "        Y_test = [Y[i] for i in test_index]\n",
        "\n",
        "        if ot_gd:\n",
        "            model = PhysioMTL(cost_func, alpha=physio_alpha, verbose=False)\n",
        "            time1=time.time()\n",
        "            model.fit(X_train, S_train, Y_train, Momentum=False)\n",
        "            time1=time.time()-time1\n",
        "            time_ot_gd.append(time1)\n",
        "            preds = model.predict(X_test, S_test)\n",
        "            mse_ot_gd.append(get_rmse(preds, Y_test))\n",
        "\n",
        "        if ot_gdm:\n",
        "            model = PhysioMTL(cost_func, alpha=physio_alpha, verbose=False)\n",
        "            time1=time.time()\n",
        "            model.fit(X_train, S_train, Y_train, Momentum=True)\n",
        "            time1=time.time()-time1\n",
        "            time_ot_gdm.append(time1)\n",
        "            preds = model.predict(X_test, S_test)\n",
        "            mse_ot_gdm.append(get_rmse(preds, Y_test))\n",
        "\n",
        "        if uot_gd:\n",
        "            model = PhysioMTL(cost_func, alpha=physio_alpha, verbose=False)\n",
        "            time1=time.time()\n",
        "            model.fit_uot(X_train, S_train, Y_train,Momentum=False)\n",
        "            time1=time.time()-time1\n",
        "            time_uot_gd.append(time1)\n",
        "            preds = model.predict(X_test, S_test)\n",
        "            mse_uot_gd.append(get_rmse(preds, Y_test))\n",
        "        if uot_gdm:\n",
        "            model = PhysioMTL(cost_func, alpha=physio_alpha, verbose=False)\n",
        "            time1=time.time()\n",
        "            model.fit_uot(X_train, S_train, Y_train,Momentum=True)\n",
        "            time1=time.time()-time1\n",
        "            time_uot_gdm.append(time1)\n",
        "            preds = model.predict(X_test, S_test)\n",
        "            mse_uot_gdm.append(get_rmse(preds, Y_test))\n",
        "\n",
        "        x = np.zeros((len(X_train), 100, 3 + 6))\n",
        "        y = np.zeros((len(X_train), 100))\n",
        "        for i, s in enumerate(S_train):\n",
        "            idx = list(range(0, X_train[i].shape[0]))\n",
        "            random.shuffle(idx)\n",
        "            x[i:i+1, :, 0:3] = X_train[i][idx[:100]]\n",
        "            y[i:i+1, :] = Y_train[i][idx[:100]].reshape((1, -1))\n",
        "            x[i:i+1, :, 3:] = s.reshape((1, -1))\n",
        "\n",
        "        x = np.zeros((len(X_train), 100, 3 + 6))\n",
        "        y = np.zeros((len(X_train), 100))\n",
        "        for i, s in enumerate(S_train):\n",
        "            idx = list(range(0, X_train[i].shape[0]))\n",
        "            random.shuffle(idx)\n",
        "            x[i:i+1, :, 0:3] = X_train[i][idx[:100]]\n",
        "            y[i:i+1, :] = Y_train[i][idx[:100]].reshape((1, -1))\n",
        "            x[i:i+1, :, 3:] = s.reshape((1, -1))\n",
        "\n",
        "\n",
        "    print(f'{percentage*100}% (alpha {compare_alpha} vs {physio_alpha})')\n",
        "\n",
        "    if ot_gd:\n",
        "        print(f'    PhysioMTL_GD:    {np.mean(mse_ot_gd)} ± {np.std(mse_ot_gd)}')\n",
        "        print(f'    Time_GD:    {np.mean(time_ot_gd)} ± {np.std(time_ot_gd)}')\n",
        "\n",
        "    if ot_gdm:\n",
        "        print(f'    PhysioMTL_GDM:    {np.mean(mse_ot_gdm)} ± {np.std(mse_ot_gdm)}')\n",
        "        print(f'    Time_GDM:    {np.mean(time_ot_gdm)} ± {np.std(time_ot_gdm)}')\n",
        "\n",
        "    if uot_gd:\n",
        "        print(f'    PhysioMTL_UOT_GD:    {np.mean(mse_uot_gd)} ± {np.std(mse_uot_gd)}')\n",
        "        print(f'    Time_UOT_GD:    {np.mean(time_uot_gd)} ± {np.std(time_uot_gd)}')\n",
        "\n",
        "    if uot_gdm:\n",
        "        print(f'    PhysioMTL_UOT_GDM:    {np.mean(mse_uot_gdm)} ± {np.std(mse_uot_gdm)}')\n",
        "        print(f'    Time_UOT_GDM:    {np.mean(time_uot_gdm)} ± {np.std(time_uot_gdm)}')\n",
        "\n",
        "\n",
        "def get_rmse(a, b):\n",
        "    return sum([np.sqrt(np.mean(np.square(a[i] - b[i]))) for i in range(len(a))]) / len(a)\n",
        "\n",
        "def get_mse(model, X_train, Y_train, S_train, X_test, S_test, Y_test, cost_func):\n",
        "    model.fit(X_train, Y_train)\n",
        "    y = []\n",
        "    for i, s_test in enumerate(S_test):\n",
        "        idx = []\n",
        "        for _, top_cost in sorted((cost_func(s, s_test), s) for s in S_train)[:1]:\n",
        "            for j, s_train in enumerate(S_train):\n",
        "                if np.max(s_train - top_cost) < 1e-6:\n",
        "                    idx.append(j)\n",
        "                    break\n",
        "\n",
        "        X = np.zeros((len(X_test[i]), 9))\n",
        "        X[:, 0:3] = X_test[i]\n",
        "        X[:, 3:] = s_test.reshape((1, -1))\n",
        "        y.append(X @ model.coef_[:, idx] + np.median(Y_train))\n",
        "\n",
        "    return get_rmse(y, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDr1rdIbE__P"
      },
      "source": [
        "## Original Evaluation\n",
        "\n",
        "Train the MMASH dataset using the author utilized alpha and Physio alpha values, among a variety of training ratios, using our PhysioMTL model as well as 5 other baseline models. Training across all models is done over 20 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QQL0BdCzeI_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7180757b-e991-4c87-c0a2-22c30e5d8f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.0% (alpha None vs 0.5)\n",
            "    PhysioMTL_GD:    30.01207663418079 ± 0.6759605243053421\n",
            "    Time_GD:    0.2924157381057739 ± 0.06678169738822465\n",
            "    PhysioMTL_GDM:    30.11251241408663 ± 0.6860428654525352\n",
            "    Time_GDM:    0.24371322393417358 ± 0.06655389325773069\n",
            "    PhysioMTL_UOT_GD:    29.948401240796073 ± 0.7119027033995701\n",
            "    Time_UOT_GD:    0.14179593324661255 ± 0.04337585257696374\n",
            "    PhysioMTL_UOT_GDM:    30.03701248367297 ± 0.6721871136010001\n",
            "    Time_UOT_GDM:    0.13691540956497192 ± 0.024345956624661727\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 36234755174 cycles (18.117377587 seconds)\n",
            "\n",
            "40.0% (alpha None vs 0.5)\n",
            "    PhysioMTL_GD:    30.11619006730868 ± 0.8508284343079405\n",
            "    Time_GD:    1.4321748852729796 ± 0.4716538043397725\n",
            "    PhysioMTL_GDM:    30.189257437451623 ± 0.8894879263934059\n",
            "    Time_GDM:    0.8241727828979493 ± 0.3915318431934659\n",
            "    PhysioMTL_UOT_GD:    30.034451797314272 ± 0.8735606706500648\n",
            "    Time_UOT_GD:    0.5678562641143798 ± 0.13873925872544496\n",
            "    PhysioMTL_UOT_GDM:    30.140188732028083 ± 0.8630774232756998\n",
            "    Time_UOT_GDM:    0.37747722864151 ± 0.09266816484844675\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 141476775298 cycles (70.738387649 seconds)\n",
            "\n",
            "60.0% (alpha None vs 0.5)\n",
            "    PhysioMTL_GD:    30.023738445986755 ± 1.2198907244443398\n",
            "    Time_GD:    3.8064934611320496 ± 0.9093454842655722\n",
            "    PhysioMTL_GDM:    30.10857969164677 ± 1.2445445128591703\n",
            "    Time_GDM:    1.7810964822769164 ± 0.9158781786899344\n",
            "    PhysioMTL_UOT_GD:    29.88318137008771 ± 1.219643519383871\n",
            "    Time_UOT_GD:    1.1092645406723023 ± 0.17777316199566015\n",
            "    PhysioMTL_UOT_GDM:    30.054335875488114 ± 1.229348642013424\n",
            "    Time_UOT_GDM:    0.770853316783905 ± 0.1746164783881148\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 329385103572 cycles (164.692551786 seconds)\n",
            "\n",
            "80.0% (alpha None vs 0.5)\n",
            "    PhysioMTL_GD:    30.377378616088215 ± 2.488007621956694\n",
            "    Time_GD:    6.624807858467102 ± 1.459711719741984\n",
            "    PhysioMTL_GDM:    30.460011346026636 ± 2.4622658251288136\n",
            "    Time_GDM:    2.4876004815101624 ± 1.1170394203198948\n",
            "    PhysioMTL_UOT_GD:    30.310474103360725 ± 2.5310749230181\n",
            "    Time_UOT_GD:    1.806068241596222 ± 0.27926459508123047\n",
            "    PhysioMTL_UOT_GDM:    30.411531469026016 ± 2.4785360921641124\n",
            "    Time_UOT_GDM:    1.1994808912277222 ± 0.19198274745612937\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 533836535202 cycles (266.918267601 seconds)\n",
            "\n",
            "20.0% (alpha None vs 0.9)\n",
            "    PhysioMTL_GD:    30.296277780155613 ± 0.9686876797106952\n",
            "    Time_GD:    0.32785797119140625 ± 0.0933956764217382\n",
            "    PhysioMTL_GDM:    30.498515432644712 ± 0.9887280636585988\n",
            "    Time_GDM:    0.2890313982963562 ± 0.07812916497022387\n",
            "    PhysioMTL_UOT_GD:    30.173592705225495 ± 0.9479836617230093\n",
            "    Time_UOT_GD:    0.16164997816085816 ± 0.04930840271491218\n",
            "    PhysioMTL_UOT_GDM:    30.352238701103477 ± 0.9724970774242051\n",
            "    Time_UOT_GDM:    0.1390299916267395 ± 0.02222395956983784\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 40741434134 cycles (20.370717067 seconds)\n",
            "\n",
            "40.0% (alpha None vs 0.9)\n",
            "    PhysioMTL_GD:    29.648434522499194 ± 0.9354350945361048\n",
            "    Time_GD:    1.5665917992591858 ± 0.42890579313605953\n",
            "    PhysioMTL_GDM:    29.808398920449104 ± 0.9049923487733346\n",
            "    Time_GDM:    0.8612083196640015 ± 0.35268774234808864\n",
            "    PhysioMTL_UOT_GD:    29.571172987257246 ± 0.9958755996458548\n",
            "    Time_UOT_GD:    0.5592602849006653 ± 0.11474441890215407\n",
            "    PhysioMTL_UOT_GDM:    29.686955567399462 ± 0.9193623535166073\n",
            "    Time_UOT_GDM:    0.3723961353302002 ± 0.08717476789639052\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 148385976402 cycles (74.192988201 seconds)\n",
            "\n",
            "60.0% (alpha None vs 0.9)\n",
            "    PhysioMTL_GD:    29.190418759448754 ± 1.345309565619871\n",
            "    Time_GD:    3.6385358572006226 ± 0.6846349863434554\n",
            "    PhysioMTL_GDM:    29.271816763853433 ± 1.2913045171668223\n",
            "    Time_GDM:    1.99822496175766 ± 0.7221502737791373\n",
            "    PhysioMTL_UOT_GD:    29.13167801138149 ± 1.3916596335693348\n",
            "    Time_UOT_GD:    1.0860292077064515 ± 0.19662364540324653\n",
            "    PhysioMTL_UOT_GDM:    29.22627917904374 ± 1.316509621004054\n",
            "    Time_UOT_GDM:    0.7361849427223206 ± 0.11146137423645572\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 328858109082 cycles (164.429054541 seconds)\n",
            "\n",
            "80.0% (alpha None vs 0.9)\n",
            "    PhysioMTL_GD:    30.50514342669095 ± 2.8753648371203298\n",
            "    Time_GD:    6.064091300964355 ± 0.9653564288046352\n",
            "    PhysioMTL_GDM:    30.584250828354243 ± 2.911381160584382\n",
            "    Time_GDM:    3.3988470077514648 ± 0.9973127825336027\n",
            "    PhysioMTL_UOT_GD:    30.4710276449188 ± 2.8573232463173763\n",
            "    Time_UOT_GD:    1.9008043169975282 ± 0.3105208233427175\n",
            "    PhysioMTL_UOT_GDM:    30.530007398347788 ± 2.894577108335354\n",
            "    Time_UOT_GDM:    1.3332641124725342 ± 0.2894635570632712\n",
            "    Evaluation:\n",
            "         iterations: 20\n",
            "         elapsed time: 559299323690 cycles (279.649661845 seconds)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate_all_percentages(mmash.X, mmash.S, mmash.Y, percentages=[0.2,0.4,0.6,0.8],physio_alpha=0.1,compare_alpha=None)\n",
        "\n",
        "evaluate_all_percentages(mmash.X, mmash.S, mmash.Y, percentages=[0.2,0.4,0.6,0.8],physio_alpha=0.5,compare_alpha=None)\n",
        "\n",
        "evaluate_all_percentages(mmash.X, mmash.S, mmash.Y, percentages=[0.2,0.4,0.6,0.8],physio_alpha=0.9,compare_alpha=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igc2tpszzXAG"
      },
      "source": [
        "#Experiment & Ablation Evaluations\n",
        "\n",
        "In each of the following sections, we attempt to recreate various claims made by authors of the original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC5UvbwbS20K"
      },
      "source": [
        "## Experiment 1: Validate Optimal Alpha\n",
        "\n",
        "When the original authors trained the 5 baseline models for evaluation against PhysioMTL, they used a fixed alpha=0.9 (learning rate) for those models and alpha=0.1 for PhysioMTL. They stated that 0.9 was the optimal value for those other models.\n",
        "\n",
        "The code within this section addresses this claim by evaluating all 5 models as well as PhysioMTL across 20 iterations, each with 20%, 40%, 60%, and 80% of the MMASH data, with 10 different alpha values (0.1 to 1.0). In another words, we evaluate all 5 models across 1,000 iterations using increasing amounts of sample data and learning rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5QV3JSl0ITh"
      },
      "source": [
        "### For Baseline Comparison Models\n",
        "\n",
        "Perform experimental ablation for 5 baseline models (Group lasso, multi-level lasso, dirty model, multi-task wasserstein, and reweighted multi-taks wasserstein learning models). Note that alpha 0.9 is not evaluated here, as it was done so in the Original Evaluation section above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWOUeLo2FLjE"
      },
      "source": [
        "## Experiment 2: Remove Denoising (Z-Score Max)\n",
        "\n",
        "The original authors omitted HRV data more than 2.5 standard deviations from the mean, as part of denoising. Within this section, we assess this decision by training all models with 20%, 40%, 60%, and 80% of MMASH without this z-score denoising, to evaluate whether PhysioMTL still performs better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwRf-lBzFaTk"
      },
      "outputs": [],
      "source": [
        "tasks = [i for i in TASKS if i not in OMIT_TASKS]\n",
        "mmash_no_zmax = MMASH(tasks, max_zscore=None)\n",
        "evaluate_all_percentages(mmash_no_zmax.X, mmash_no_zmax.S, mmash_no_zmax.Y, percentages=[0.2, 0.4, 0.6, 0.8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXBPt5QvKdZJ"
      },
      "source": [
        "## Experiment 3: Omit Unclean Sample Data\n",
        "\n",
        "According to the original paper, the MMASH data set had some patients who were missing sleep and age data, so the authors coded assumed sleep and age numbers into the preprocessing. Doing this potentially risks confirming the authors’ biases that demographic data has very strong correlation to HRV, especially since MMASH is a relatively small data set. Thus, the code in this section performs an experiment in which we evaluate all models across 20%, 40%, 60%, and 80% of the MMASH data by excluding those patients, to see whether PhysioMTL still performs better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff0aLtjDKmOc"
      },
      "outputs": [],
      "source": [
        "tasks = [i for i in TASKS if i not in OMIT_TASKS and i not in OVERWRITES]\n",
        "mmash_no_unclean = MMASH(tasks, max_zscore=None)\n",
        "evaluate_all_percentages(mmash_no_unclean.X, mmash_no_unclean.S, mmash_no_unclean.Y, percentages=[0.2, 0.4, 0.6, 0.8],compare_alpha=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMzi1aAz2Hf2"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Zhu, Jiacheng et al. (July 2022). “PhysioMTL: Personalizing Physiological Patterns using Optimal Transport Multi-Task Regression”. In: Proceedings of the Conference on Health, Inference, and Learning. Ed. by Gerardo Flores et al. Vol. 174. Proceedings of Machine Learning Research. PMLR, pp. 354–374. URL: https://proceedings.mlr.press/v174/zhu22a.html.\n",
        "\n",
        "[2] Acharya, Rajendra U. et al. (2006). “Heart Rate Variability: A Review”. In: Medical and Biological Engineering and Computing 44.12, pp. 1031–1051.\n",
        "\n",
        "[3] Rossi, Alessio et al. (2020). Multilevel Monitoring of Activity and Sleep in Healthy People (MMASH). version 1.0.0. URL: https://doi.org/10.13026/cerq-fc86.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}